{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GMM-UBM Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](docs/img/GMM-UBM.png)\n",
    "\n",
    "\n",
    "To extract unbiased measures of convergence, we used a data-driven, text-independent, automatic speaker identification technique, based on Gaussian Markov Modeling (GMM) Universal Background Model\n",
    "(UBM). The Gaussian components model the underlying broad phonetic features (i.e., MFCCs) that characterize a speaker's voice and are based on a well-understood statistical model. \n",
    "\n",
    "A 32-component UBM was trained with the pooled Solo_Pre speech data of all the participants. Then, individual speaker-dependent models were obtained via maximum a posteriori (MAP) adaptation of the UBMs to the Solo_Pre speech data of each speaker separately. The GMM-UBM has multiple hyperparameters and different settings of these hyperparameters can affect the performance of speaker-dependent models.\n",
    "\n",
    "A cross-validation technique was used to choose the optimum hyper-parameter settings. Solo_Post speech data were used as a validation set, and each speaker-dependent model's performance was verified against the UBM model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "code_folding": [
     0
    ],
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# init\n",
    "import numpy as np\n",
    "import os\n",
    "import sidekit\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import gmm_scoring_singleThread\n",
    "from sidekit.bosaris import Scores\n",
    "from IPython.display import clear_output\n",
    "\n",
    "#read from wav folder, extract mfcc and save it on audio_features folder. train ubm and speaker \n",
    "#model, save it on data folder. finally get the prediction score and save it on data folder. \n",
    "#use HDFCompass.exe (https://support.hdfgroup.org/projects/compass/download.html) \n",
    "#to see whats inside the .h5 files and modify accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# load wav files and extract features and save it to folder\n",
    "audioDir = 'wav'\n",
    "featureDir = 'audio_features'\n",
    "\n",
    "speakers = np.array([\"elvira\", \"marion\"])\n",
    "\n",
    "\n",
    "# load wav files\n",
    "fileList = os.listdir(audioDir)\n",
    "for i in range(0,len(fileList)):\n",
    "    fileList[i] = fileList[i].replace(\".wav\", \"\")\n",
    "    print(fileList[i])\n",
    "\n",
    "\n",
    "\n",
    "# feature extraction configuration (read from fileList and save mfcc features in \n",
    "# audio_features folder)\n",
    "extractor = sidekit.FeaturesExtractor(audio_filename_structure=audioDir+\"/{}.wav\",\n",
    "                                      feature_filename_structure=\"./audio_features/{}.h5\",\n",
    "                                      sampling_frequency=22050,\n",
    "                                      lower_frequency=0,\n",
    "                                      higher_frequency=6955.4976,\n",
    "                                      filter_bank=\"log\",\n",
    "                                      filter_bank_size=40,\n",
    "                                      window_size=0.025,\n",
    "                                      shift=0.01,\n",
    "                                      ceps_number=6,\n",
    "                                      pre_emphasis=0.97,\n",
    "                                      save_param=[\"energy\", \"cep\"],\n",
    "                                      keep_all_features=True)\n",
    "\n",
    "\n",
    "for i in range(0,len(fileList)):\n",
    "    a = './audio_features/' + fileList[i] +'.h5'\n",
    "    try:\n",
    "        os.remove(a)\n",
    "    except OSError:\n",
    "        pass\n",
    "    extractor.save(fileList[i])\n",
    "    \n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GMM-UBM (Trainnig)\n",
    "\n",
    "file labels for traning validation and testing\n",
    "train = *-pre , \n",
    "validation  = *-post , \n",
    "test  = *-duet"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "#create list of files for UBM traning and save it in data/UBM_List.txt\n",
    "ubmList =  []\n",
    "for i in range(0,len(fileList)):\n",
    "    a = fileList[i].replace('.h5', '')\n",
    "    b = fileList[i].split('-')\n",
    "    if b[1][1]== 'r':#pre or train\n",
    "        ubmList.append(a)\n",
    "        \n",
    "with open('data/UBM_List.txt','w') as of:\n",
    "    of.write(\"\\n\".join(ubmList))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "#create traning files and modify them to sidekit file format (i.e. IdMap)\n",
    "train_subjects_models = []\n",
    "train_audio_files = []\n",
    "for i in range(0,len(fileList)):\n",
    "    a = fileList[i].replace('.h5', '')\n",
    "    b = fileList[i].split('-')\n",
    "    if b[1][1]== 'r':#pre or train\n",
    "        train_subjects_models.append(b[0])\n",
    "        train_audio_files.append(a)\n",
    "        \n",
    "    \n",
    "# Create and fill the IdMap\n",
    "train_idmap = sidekit.IdMap()\n",
    "train_idmap.leftids = np.asarray(train_subjects_models)\n",
    "train_idmap.rightids = np.asarray(train_audio_files)\n",
    "train_idmap.start = np.empty(train_idmap.rightids.shape, '|O')\n",
    "train_idmap.stop = np.empty(train_idmap.rightids.shape, '|O')\n",
    "train_idmap.validate()\n",
    "train_idmap.write('data/train.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "#create validation files and modify them to sidekit file format (i.e. IdMap, key)\n",
    "validate_audio_files = []\n",
    "validate_subjects_models = []\n",
    "for i in range(0,len(fileList)):\n",
    "    a = fileList[i].replace('.h5', '')\n",
    "    b = fileList[i].split('-')\n",
    "    if b[1][1]== 'o':#post\n",
    "        validate_subjects_models.append(b[0])  \n",
    "        validate_audio_files.append(a)\n",
    "        \n",
    "        \n",
    "ndx = sidekit.Ndx()\n",
    "ndx.modelset = speakers\n",
    "ndx.segset = np.array(validate_audio_files)\n",
    "ndx.trialmask = np.ones((len(speakers),len(ndx.segset)), dtype='bool')\n",
    "ndx.validate()           \n",
    "ndx.write('data/validation.h5')\n",
    "\n",
    "segments = [i.split('-', 1)[0] for i in ndx.segset]\n",
    "seg_index = np.arange(0,len(ndx.segset))\n",
    "\n",
    "key = sidekit.Key()\n",
    "key.modelset = ndx.modelset\n",
    "key.segset = ndx.segset\n",
    "key.tar = np.zeros((len(speakers),len(ndx.segset)), dtype='bool')\n",
    "key.non = np.zeros((len(speakers),len(ndx.segset)), dtype='bool')\n",
    "for s in range(0,len(key.modelset)):\n",
    "    a = [i for i, x in enumerate(segments) if x == key.modelset[s]]\n",
    "    a = np.asarray(a)\n",
    "    key.tar[s, a] = True\n",
    "    b = np.delete(seg_index, a)    \n",
    "    key.non[s, b] = True\n",
    "\n",
    "key.validate()\n",
    "key.write('data/validation_key.h5')    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "## Create a FeaturesServer to load features and feed the other methods\n",
    "features_server = sidekit.FeaturesServer(features_extractor=None,\n",
    "                                         feature_filename_structure=\"./audio_features/{}.h5\",\n",
    "                                         sources=None,\n",
    "                                         dataset_list=[\"energy\", \"cep\"],\n",
    "                                         mask=\"[0-5]\",\n",
    "                                         feat_norm=\"cmvn\",\n",
    "                                         global_cmvn=None,\n",
    "                                         dct_pca=False,\n",
    "                                         dct_pca_config=None,\n",
    "                                         sdc=False,\n",
    "                                         sdc_config=None,\n",
    "                                         delta=True,\n",
    "                                         double_delta=True,\n",
    "                                         delta_filter=None,\n",
    "                                         context=None,\n",
    "                                         traps_dct_nb=None,\n",
    "                                         rasta=True,\n",
    "                                         keep_all_features=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# load train, validation, validation_key, UBM files for GMM-UBM traning\n",
    "train_idmap = sidekit.IdMap('data/train.h5')\n",
    "validation_ndx = sidekit.Ndx('data/validation.h5')\n",
    "key = sidekit.Key('data/validation_key.h5')\n",
    "with open('data/UBM_List.txt') as inputFile:\n",
    "        ubmList = inputFile.read().split('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GMM-UBM (Testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (hyperparameters)\n",
    "distribNb = 32 # no of GMM components \n",
    "regulation_factor = 3  # MAP regulation factor "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# train UBM\n",
    "ubm = sidekit.Mixture()\n",
    "llk = ubm.EM_split(features_server, ubmList, distribNb, save_partial=False)\n",
    "ubm.write('data/ubm_model.h5')\n",
    "#ubm.read('data/ubm_model.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# this is a workaround because the above process gets stuck sometime\n",
    "# sidekit uses multiprocess so some time it hangs (need to search a properway to kill \n",
    "# all the python spawned process)\n",
    "ubm = sidekit.Mixture()\n",
    "ubm.read('data/ubm_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# train speaker model (Adapt the GMM speaker models from the UBM via a MAP adaptation)\n",
    "enroll_stat = sidekit.StatServer(train_idmap,distrib_nb=distribNb,feature_size=18)\n",
    "enroll_stat.accumulate_stat(ubm=ubm,feature_server=features_server, seg_indices=range(enroll_stat.segset.shape[0]))\n",
    "enroll_stat.write('data/stat_enroll.h5')\n",
    "\n",
    "# \n",
    "enroll_sv = enroll_stat.adapt_mean_map_multisession(ubm, regulation_factor)\n",
    "enroll_sv.write('data/sv_enroll.h5')\n",
    "#enroll_sv.read('data/sv_enroll.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# predict on the test data and save it on scores.h5\n",
    "s = np.zeros(validation_ndx.trialmask.shape)\n",
    "los = np.array_split(np.arange(validation_ndx.segset.shape[0]),1)\n",
    "for idx in los:\n",
    "    gmm_scoring_singleThread.gmm_scoring_singleThread(ubm, enroll_sv, validation_ndx, features_server, s,idx)\n",
    "\n",
    "\n",
    "score = Scores()\n",
    "score.scoremat = s\n",
    "score.modelset = validation_ndx.modelset\n",
    "score.segset = validation_ndx.segset\n",
    "score.scoremask = validation_ndx.trialmask\n",
    "score.write('data/scores.h5')   \n",
    "\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
